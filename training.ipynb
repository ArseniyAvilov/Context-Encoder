{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Подключение к tensorboared","metadata":{}},{"cell_type":"code","source":"# using the hard way \n# run tensorboard in kaggle server. and operate using public url\n\n# download the files for ngrok\n!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n!unzip ngrok-stable-linux-amd64.zip\n\n# Run tensorboard as well as Ngrox (for tunneling as non-blocking processes)\nimport os\nimport multiprocessing\n\n\npool = multiprocessing.Pool(processes = 10)\nresults_of_processes = [pool.apply_async(os.system, args=(cmd, ), callback = None )\n                        for cmd in [\n                        f\"tensorboard --logdir ./runs/ --host 0.0.0.0 --port 6006 &\",\n                        \"./ngrok http 6006 &\"\n                        ]]\n","metadata":{"execution":{"iopub.status.busy":"2021-07-12T09:42:41.151207Z","iopub.execute_input":"2021-07-12T09:42:41.151536Z","iopub.status.idle":"2021-07-12T09:42:43.374191Z","shell.execute_reply.started":"2021-07-12T09:42:41.151509Z","shell.execute_reply":"2021-07-12T09:42:43.372425Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"--2021-07-12 09:42:41--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\nResolving bin.equinox.io (bin.equinox.io)... 3.212.129.206, 52.45.159.115, 3.95.52.0, ...\nConnecting to bin.equinox.io (bin.equinox.io)|3.212.129.206|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 13832437 (13M) [application/octet-stream]\nSaving to: ‘ngrok-stable-linux-amd64.zip’\n\nngrok-stable-linux- 100%[===================>]  13.19M  40.5MB/s    in 0.3s    \n\n2021-07-12 09:42:42 (40.5 MB/s) - ‘ngrok-stable-linux-amd64.zip’ saved [13832437/13832437]\n\nArchive:  ngrok-stable-linux-amd64.zip\n  inflating: ngrok                   \n","output_type":"stream"}]},{"cell_type":"code","source":"! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\"","metadata":{"execution":{"iopub.status.busy":"2021-07-12T09:42:43.382821Z","iopub.execute_input":"2021-07-12T09:42:43.388691Z","iopub.status.idle":"2021-07-12T09:42:44.657601Z","shell.execute_reply.started":"2021-07-12T09:42:43.388641Z","shell.execute_reply":"2021-07-12T09:42:44.655897Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"http://9e982c606afc.ngrok.io\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os, math, sys\nimport glob\nimport random\n\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader, Dataset, ConcatDataset\nfrom torchvision.utils import save_image, make_grid\n\nimport matplotlib.pyplot as plt\n\nfrom PIL import Image\nfrom tqdm import tqdm_notebook as tqdm\n\nrandom.seed(42)\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2021-07-12T09:42:45.576364Z","iopub.execute_input":"2021-07-12T09:42:45.576726Z","iopub.status.idle":"2021-07-12T09:42:47.016844Z","shell.execute_reply.started":"2021-07-12T09:42:45.576693Z","shell.execute_reply":"2021-07-12T09:42:47.015889Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# number of epochs of training\nn_epochs = 50\n# size of the batches\nbatch_size = 64\n# name of the dataset\ndataset_name = \"../input/val-256/val_256\"\n# adam: learning rate\nlr = 10**-4\n# adam: decay of first order momentum of gradient\nb1 = 0.5\n# adam: decay of first order momentum of gradient\nb2 = 0.999\n# number of cpu threads to use during batch generation\nn_cpu = 8\n# size of each image dimension\nimg_size = 128\n# size of random mask\nmask_size = 16\n# number of image channels\nchannels = 3\n\n# Calculate output dims of image discriminator (PatchGAN)\npatch_h, patch_w = int(mask_size / 2 ** 3), int(mask_size / 2 ** 3)\npatch = (1, patch_h, patch_w)","metadata":{"execution":{"iopub.status.busy":"2021-07-12T09:42:47.021849Z","iopub.execute_input":"2021-07-12T09:42:47.024015Z","iopub.status.idle":"2021-07-12T09:42:47.033783Z","shell.execute_reply.started":"2021-07-12T09:42:47.023973Z","shell.execute_reply":"2021-07-12T09:42:47.032831Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"### Define Dataset Class","metadata":{}},{"cell_type":"code","source":"class ImageDataset(Dataset):\n    def __init__(self, root, transforms_=None, img_size=128, mask_size=16, mode=\"train\"):\n        self.transform = transforms.Compose(transforms_)\n        self.img_size = img_size\n        self.mask_size = mask_size\n        self.mode = mode\n        self.files = sorted(glob.glob(\"%s/*.jpg\" % root))\n        # self.files = self.files[:-4000] if mode == \"train\" else self.files[-4000:]\n\n    def apply_random_mask(self, img):\n        \"\"\"Randomly masks image\"\"\"\n        y1, x1 = np.random.randint(0, self.img_size - self.mask_size, 2)\n        y2, x2 = y1 + self.mask_size, x1 + self.mask_size\n        masked_part = img[:, y1:y2, x1:x2]\n        masked_img = img.clone()\n        masked_img[:, y1:y2, x1:x2] = 1\n\n        return masked_img, masked_part\n\n    def __getitem__(self, index):\n\n        img = Image.open(self.files[index % len(self.files)])\n        img = self.transform(img)\n        if int(transforms.ToTensor()(img).shape[0]) == 1:\n            img = transforms.Grayscale(num_output_channels=3)(img)\n        \n        img = transforms.Compose([transforms.ToTensor(),\n                                  transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),])(img)\n\n        masked_img, aux = self.apply_random_mask(img)\n\n        return img, masked_img, aux\n\n    def __len__(self):\n        return len(self.files)","metadata":{"execution":{"iopub.status.busy":"2021-07-12T09:42:50.319390Z","iopub.execute_input":"2021-07-12T09:42:50.319809Z","iopub.status.idle":"2021-07-12T09:42:50.340342Z","shell.execute_reply.started":"2021-07-12T09:42:50.319761Z","shell.execute_reply":"2021-07-12T09:42:50.339345Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"transform = [\n     transforms.Resize((128, 128)),\n     transforms.ColorJitter(hue=.50, saturation=.50),\n     transforms.RandomHorizontalFlip(p=0.5),\n     transforms.RandomVerticalFlip(p=0.5),\n     ]\nos.makedirs(\"aug_data\", exist_ok=True)\naugmentation_data = ImageDataset(dataset_name, transforms_=transform)","metadata":{"execution":{"iopub.status.busy":"2021-07-12T09:42:50.716968Z","iopub.execute_input":"2021-07-12T09:42:50.717537Z","iopub.status.idle":"2021-07-12T09:42:52.116752Z","shell.execute_reply.started":"2021-07-12T09:42:50.717493Z","shell.execute_reply":"2021-07-12T09:42:52.115804Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"transform = [\n     transforms.Resize((128, 128)),\n     transforms.ColorJitter(hue=.20, saturation=.20),\n     transforms.RandomHorizontalFlip(p=0.5),\n     transforms.RandomVerticalFlip(p=0.5),\n     ]\nos.makedirs(\"aug_data\", exist_ok=True)\naugmentation_data_1 = ImageDataset(dataset_name, transforms_=transform)","metadata":{"execution":{"iopub.status.busy":"2021-07-12T09:42:52.119361Z","iopub.execute_input":"2021-07-12T09:42:52.119911Z","iopub.status.idle":"2021-07-12T09:42:52.257927Z","shell.execute_reply.started":"2021-07-12T09:42:52.119872Z","shell.execute_reply":"2021-07-12T09:42:52.257231Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"### Get Train/Test Dataloaders","metadata":{}},{"cell_type":"code","source":"transforms_ = [\n    transforms.Resize((img_size, img_size), Image.BICUBIC),\n    #transforms.ToTensor(),\n    #transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n]\n\ndataloader = DataLoader(\n    ConcatDataset([ImageDataset(dataset_name, transforms_=transforms_),augmentation_data_1, augmentation_data]),\n    batch_size=batch_size,\n    shuffle=True,\n    num_workers=n_cpu,\n)","metadata":{"execution":{"iopub.status.busy":"2021-07-12T09:42:52.259409Z","iopub.execute_input":"2021-07-12T09:42:52.259901Z","iopub.status.idle":"2021-07-12T09:42:52.397421Z","shell.execute_reply.started":"2021-07-12T09:42:52.259863Z","shell.execute_reply":"2021-07-12T09:42:52.396696Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"<h3><center>Model Architecture</center></h3>\n<img src=\"https://miro.medium.com/max/700/1*fJpamgw0yBZZRNEuex07hw.png\" width=\"1000\" height=\"1000\"/>\n<h4></h4>\n<h4><center>Image Source:  <a href=\"https://arxiv.org/abs/1609.04802\">Context Encoders: Feature Learning by Inpainting [Deepak Pathak et al.]</a></center></h4>","metadata":{}},{"cell_type":"code","source":"class Generator(nn.Module):\n    def __init__(self, channels=3):\n        super(Generator, self).__init__()\n\n        def encoder(in_feat, out_feat, normalize=True):\n            layers = [nn.Conv2d(in_feat, out_feat, 4, stride=2, padding=1)]\n            if normalize:\n                layers.append(nn.BatchNorm2d(out_feat, 0.8))\n            layers.append(nn.LeakyReLU(0.2))\n            return layers\n\n        def decoder(in_feat, out_feat, normalize=True):\n            layers = [nn.ConvTranspose2d(in_feat, out_feat, 4, stride=2, padding=1)]\n            if normalize:\n                layers.append(nn.BatchNorm2d(out_feat, 0.8))\n            layers.append(nn.ReLU())\n            return layers\n\n        self.model = nn.Sequential(\n            *encoder(channels, 64, normalize=False),\n            *encoder(64, 64),\n            *encoder(64, 128),\n            *encoder(128, 256),\n            *encoder(256, 512),\n            nn.Conv2d(512, 4000, 1),\n            *decoder(4000, 512),\n            *decoder(512, 256),\n            *decoder(256, 128),\n            *decoder(128, 64),\n            nn.Conv2d(64, channels, 3, 1, 1),\n            nn.Tanh()\n        )\n\n    def forward(self, x):\n        return self.model(x)\n    \n\nclass Discriminator(nn.Module):\n    def __init__(self, channels=3):\n        super(Discriminator, self).__init__()\n\n        def discriminator_block(in_filters, out_filters, stride, normalize):\n            layers = [nn.Conv2d(in_filters, out_filters, 3, stride, 1)]\n            if normalize:\n                layers.append(nn.InstanceNorm2d(out_filters))\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\n            return layers\n\n        self.model = nn.Sequential(*discriminator_block(channels, 64, 2, False),\n                                   *discriminator_block(64, 128, 2, True),\n                                   *discriminator_block(128, 256, 2, True),\n                                   *discriminator_block(256, 512, 1, True),\n                                   nn.Conv2d(512, 1, 3, 1, 1))\n\n    def forward(self, img):\n        return self.model(img)","metadata":{"execution":{"iopub.status.busy":"2021-07-12T09:42:52.538824Z","iopub.execute_input":"2021-07-12T09:42:52.539116Z","iopub.status.idle":"2021-07-12T09:42:52.561119Z","shell.execute_reply.started":"2021-07-12T09:42:52.539086Z","shell.execute_reply":"2021-07-12T09:42:52.560354Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"len(dataloader)","metadata":{"execution":{"iopub.status.busy":"2021-07-12T09:42:52.931447Z","iopub.execute_input":"2021-07-12T09:42:52.931772Z","iopub.status.idle":"2021-07-12T09:42:52.939263Z","shell.execute_reply.started":"2021-07-12T09:42:52.931740Z","shell.execute_reply":"2021-07-12T09:42:52.938230Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"1711"},"metadata":{}}]},{"cell_type":"markdown","source":"### Train Context-Encoder GAN","metadata":{}},{"cell_type":"code","source":"def weights_init_normal(m):\n    classname = m.__class__.__name__\n    if classname.find(\"Conv\") != -1:\n        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n    elif classname.find(\"BatchNorm2d\") != -1:\n        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n        torch.nn.init.constant_(m.bias.data, 0.0)\n    \n# Loss function\nadversarial_loss = torch.nn.MSELoss()\npixelwise_loss = torch.nn.L1Loss()\n\n# Initialize generator and discriminator\ngenerator = Generator(channels=channels)\ndiscriminator = Discriminator(channels=channels)\n\n# Load pretrained models\n# generator.load_state_dict(torch.load(\"../input/context-encoder-gan-for-image-inpainting-pytorch/saved_models/generator.pth\"))\n# discriminator.load_state_dict(torch.load(\"../input/context-encoder-gan-for-image-inpainting-pytorch/saved_models/discriminator.pth\"))\n# print(\"Using pre-trained Context-Encoder GAN model!\")\n\n\ngenerator.model[27] = nn.Conv2d(64, 32, 4, stride=2, padding=1)\ngenerator.model[28] = nn.BatchNorm2d(32, 0.8)\n\ngenerator = nn.Sequential(generator,\n                          nn.ReLU(),\n                          nn.Conv2d(32, 16, 4, stride=2, padding=1),\n                          nn.BatchNorm2d(16, 0.8),\n                          nn.Conv2d(16, 3, 3, 1, 1),\n                          nn.Tanh()\n        )\n\n\ngenerator.cuda()\ndiscriminator.cuda()\nadversarial_loss.cuda()\npixelwise_loss.cuda()\n\n# Initialize weights\ngenerator.apply(weights_init_normal)\ndiscriminator.apply(weights_init_normal)\n\n# Optimizers\noptimizer_G = torch.optim.Adam(generator.parameters(), lr=lr, betas=(b1, b2))\noptimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(b1, b2))\n\nTensor = torch.cuda.FloatTensor","metadata":{"execution":{"iopub.status.busy":"2021-07-12T09:42:53.971646Z","iopub.execute_input":"2021-07-12T09:42:53.972186Z","iopub.status.idle":"2021-07-12T09:42:58.124504Z","shell.execute_reply.started":"2021-07-12T09:42:53.972124Z","shell.execute_reply":"2021-07-12T09:42:58.123703Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"from torch.utils.tensorboard import SummaryWriter\nwriter = SummaryWriter('runs/context_encoder')\nos.makedirs(\"saved_models\", exist_ok=True)\n\nfor epoch in range(n_epochs):\n    \n    ### Training ###\n    gen_adv_loss, gen_pixel_loss, disc_loss, gen_loss = 0, 0, 0, 0\n    tqdm_bar = tqdm(dataloader, desc=f'Training Epoch {epoch} ', total=int(len(dataloader)))\n    for i, (imgs, masked_imgs, masked_parts) in enumerate(tqdm_bar):\n\n        # Adversarial ground truths\n        valid = Variable(Tensor(imgs.shape[0], *patch).fill_(1.0), requires_grad=False)\n        fake = Variable(Tensor(imgs.shape[0], *patch).fill_(0.0), requires_grad=False)\n\n        # Configure input\n        imgs = Variable(imgs.type(Tensor))\n        masked_imgs = Variable(masked_imgs.type(Tensor))\n        masked_parts = Variable(masked_parts.type(Tensor))\n\n        ## Train Generator ##\n        optimizer_G.zero_grad()\n\n        # Generate a batch of images\n        gen_parts = generator(masked_imgs)\n\n\n        # Adversarial and pixelwise loss\n        g_adv = adversarial_loss(discriminator(gen_parts), valid)\n        g_pixel = pixelwise_loss(gen_parts, masked_parts)\n        # Total loss\n        g_loss = 0.001 * g_adv + 0.999 * g_pixel\n\n        g_loss.backward()\n        optimizer_G.step()\n\n        ## Train Discriminator ##\n        optimizer_D.zero_grad()\n\n        # Measure discriminator's ability to classify real from generated samples\n        real_loss = adversarial_loss(discriminator(masked_parts), valid)\n        fake_loss = adversarial_loss(discriminator(gen_parts.detach()), fake)\n        d_loss = 0.5 * (real_loss + fake_loss)\n\n        d_loss.backward()\n        optimizer_D.step()\n        \n\n        \n        gen_adv_loss += g_adv.item()\n        gen_pixel_loss += g_pixel.item()\n        disc_loss += d_loss.item()\n        gen_loss += g_loss.item()\n        tqdm_bar.set_postfix(gen_adv_loss=gen_adv_loss/(i+1), gen_pixel_loss=gen_pixel_loss/(i+1), disc_loss=disc_loss/(i+1))\n        if i % 50 == 49:\n            writer.add_scalar('generator loss',\n                            gen_loss/(i+1),\n                            epoch * len(dataloader) + i)\n        \n            writer.add_scalar('descremenator loss',\n                            disc_loss/(i+1),\n                            epoch * len(dataloader) + i)\n        \n         \n    torch.save(generator.state_dict(), \"saved_models/generator.pth\")\n    torch.save(discriminator.state_dict(), \"saved_models/discriminator.pth\")\nwriter.close()  ","metadata":{"execution":{"iopub.status.busy":"2021-07-12T09:42:58.126120Z","iopub.execute_input":"2021-07-12T09:42:58.126437Z"},"trusted":true},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Training Epoch 0 ', max=1711.0, style=ProgressStyle(descr…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1d258f690e794173a4382ecc9e5d857b"}},"metadata":{}},{"name":"stdout","text":"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Training Epoch 1 ', max=1711.0, style=ProgressStyle(descr…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e9df0e2835724ff9803eb2bcfa07c2a1"}},"metadata":{}},{"name":"stdout","text":"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Training Epoch 2 ', max=1711.0, style=ProgressStyle(descr…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"60fe07ccd3fa437993c6b73cf03116fd"}},"metadata":{}},{"name":"stdout","text":"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Training Epoch 3 ', max=1711.0, style=ProgressStyle(descr…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e330c5af1c545dc9ec939624bf01e29"}},"metadata":{}},{"name":"stdout","text":"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Training Epoch 4 ', max=1711.0, style=ProgressStyle(descr…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1213eb54495b4632b2c61606ac24e3ec"}},"metadata":{}},{"name":"stdout","text":"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Training Epoch 5 ', max=1711.0, style=ProgressStyle(descr…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5fc8970a0b2c47b5aac825b543aace0f"}},"metadata":{}},{"name":"stdout","text":"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Training Epoch 6 ', max=1711.0, style=ProgressStyle(descr…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f83db1143a2f443980e11f44e375a554"}},"metadata":{}},{"name":"stdout","text":"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Training Epoch 7 ', max=1711.0, style=ProgressStyle(descr…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4c5cdec6129f436d990d8aed1bba1b7a"}},"metadata":{}},{"name":"stdout","text":"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Training Epoch 8 ', max=1711.0, style=ProgressStyle(descr…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e1995e53a9f4d20be047cf772c44a13"}},"metadata":{}},{"name":"stdout","text":"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Training Epoch 9 ', max=1711.0, style=ProgressStyle(descr…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d7c432c2af54d71b487044bdf18dc46"}},"metadata":{}},{"name":"stdout","text":"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Training Epoch 10 ', max=1711.0, style=ProgressStyle(desc…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ad62322a52a44ae9acbf6568ee1a1799"}},"metadata":{}},{"name":"stdout","text":"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Training Epoch 11 ', max=1711.0, style=ProgressStyle(desc…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1452ce3ecea14174a722f930ebcf6942"}},"metadata":{}},{"name":"stdout","text":"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Training Epoch 12 ', max=1711.0, style=ProgressStyle(desc…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2e0aff7c5f6f49fa83c52afcf145249a"}},"metadata":{}},{"name":"stdout","text":"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Training Epoch 13 ', max=1711.0, style=ProgressStyle(desc…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1510719540bd43e19cd62ea2882451a4"}},"metadata":{}},{"name":"stdout","text":"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Training Epoch 14 ', max=1711.0, style=ProgressStyle(desc…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2c1cbc04ad6044a5bd9cbe269160383b"}},"metadata":{}},{"name":"stdout","text":"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Training Epoch 15 ', max=1711.0, style=ProgressStyle(desc…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ec28e5c79044ad38f978a06315d0034"}},"metadata":{}},{"name":"stdout","text":"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Training Epoch 16 ', max=1711.0, style=ProgressStyle(desc…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d8bacc067f5547648060629bf48cd897"}},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}